{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI has numerous business applications across various industries, including:\n",
      "\n",
      "1. **Marketing and Advertising**: Generative AI can help create personalized product recommendations, social media content, and even entire marketing campaigns. For example, companies like Google and Netflix use generative AI to suggest customized advertisements and video content.\n",
      "2. **Customer Service**: Chatbots powered by generative AI can provide 24/7 customer support, helping to resolve issues more efficiently and reducing the workload of human customer service agents.\n",
      "3. **Content Generation**: Generative AI can produce high-quality content such as blog posts, social media posts, product descriptions, and even entire articles. Companies like Buzzfeed, Forbes, and The New York Times have already leveraged this technology to generate engaging content.\n",
      "4. **Product Design**: Generative AI can help design new products, such as furniture, fashion items, or even architectural designs. This technology uses machine learning algorithms to create unique product concepts based on market trends, consumer preferences, and brand identities.\n",
      "5. **Predictive Maintenance**: Generative AI can analyze sensor data from machines and equipment to predict when maintenance is required, reducing downtime and increasing overall efficiency.\n",
      "6. **Supply Chain Optimization**: Generative AI can help optimize supply chain management by predicting demand, managing inventory levels, and identifying potential bottlenecks in the logistics process.\n",
      "7. **Human Capital Management**: Generative AI can be used to analyze employee data, identify talent gaps, and suggest training programs to enhance skillsets and boost employee productivity.\n",
      "8. **Accounting and Financial Analysis**: Generative AI can help automate financial reporting, accounting, and analysis tasks, freeing up time for human accountants to focus on high value-added activities.\n",
      "9. **Risk Management**: Generative AI can analyze large datasets to identify potential risks and suggest mitigation strategies to minimize potential losses or damages.\n",
      "10. **Urban Planning**: Generative AI can help design more sustainable cities by analyzing demographic data, building codes, and environmental factors to create efficient urban planning models.\n",
      "11. **Education Technology**: Generative AI can help develop personalized learning pathways for students, providing tailored content and assessments based on individual performance and learning needs.\n",
      "12. **Virtual Event Planning**: Generative AI can assist with virtual event planning, helping to create customized agendas, coordinate logistics, and manage attendee experiences.\n",
      "\n",
      "These are just a few examples of the many business applications of generative AI. As the technology continues to evolve, we can expect to see even more innovative uses in various industries and sectors.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=\"llama3.2\",\n",
    "    messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Describe some of the business applications of Generative AI\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to define some core concepts related to Large Language Models (LLMs), specifically focusing on neural networks, attention, and transformers. Hmm, where do I start?\n",
      "\n",
      "First off, I remember that neural networks are a big part of LLMs. They're inspired by the human brain, right? So they have layers: input, hidden, output. Each layer has neurons connected in layers... but wait, what's a neuron like here? Oh yeah, each neuron takes inputs, processes them with some weights and bias, applies an activation function, and passes it on.\n",
      "\n",
      "Neural networks learn through backpropagation. So during training, the model makes predictions, compares them to actual data (with a cost function), then adjusts its weights to minimize error. That way, over time, it gets better at making accurate predictions or decisions.\n",
      "\n",
      "Then there's deep learning. That's when neural networks have multiple hidden layers, making them \"deep.\" It can capture complex patterns which simpler models might miss.\n",
      "\n",
      "Transformer architectures are new to me compared to older models like RNNs and LSTMs. From what I've read, transformers use self-attention mechanisms instead of recurrence or convolution. Self-attention allows the model to weigh different words in a sentence differently when processing it, which helps with tasks where distant words can influence the output, unlike LSTMs where there's memory but more about local context.\n",
      "\n",
      "Wait, the term \"self-attention\" – does that mean each word attends to others? So like, for each position, the model looks at all previous positions to compute attention weights. This could allow attending to longer-range dependencies and handling sequences in a parallel way since it doesn't process step-by-step like RNNs.\n",
      "\n",
      "But how do these concepts tie into LLMs? LLMs are trained on vast amounts of text data, learning to predict the next word given previous context. They use all these advanced structures to get better performance than earlier models.\n",
      "\n",
      "I think I might have mixed up some details. Let me check: Transformers have multi-head attention. Each head focuses on different aspects, reducing computational load while still capturing various attention patterns. Also, the concept of dot product for similarity and scaling values after applying softmax. Oh right, that's how attention scores are calculated and normalized.\n",
      "\n",
      "But wait, I'm not sure if \"self-attention\" is exactly the same as attention in other contexts or in traditional RNN/LSTM models. In transformers, self-attention refers to attention over the input sequence itself, whereas in older models like LSTMs, it's more about memory states.\n",
      "\n",
      "Maybe LLMs leverage these structures because they allow for more efficient and effective processing of contextual information compared to previous architectures, leading to better language understanding and generation.\n",
      "\n",
      "I should make sure I explain each concept clearly without getting too bogged down in details. For the user, a solid understanding of these core concepts is key—so focus on clear definitions with maybe an example or brief context for each.\n",
      "</think>\n",
      "\n",
      "**Core Concepts in Large Language Models (LLMs): Neural Networks, Attention, Transformers**\n",
      "\n",
      "1. **Neural Network:**\n",
      "   - **Definition:** A computational model inspired by the human brain, composed of interconnected nodes (neurons) organized into layers—input, hidden, and output. Each neuron processes inputs, applies weights and biases, and uses an activation function to pass information.\n",
      "   - **Structure:** Layers are connected sequentially; input data flows through these layers to produce an output. Neurons compute transformations using mathematical functions followed by activation functions like ReLU or sigmoid.\n",
      "\n",
      "2. **Deep Learning:**\n",
      "   - **Definition:** A subset of machine learning that enables complex models, particularly neural networks with multiple hidden layers.\n",
      "   - **Significance:** Deep learning captures intricate patterns and relationships in data, improving performance on tasks requiring deep contextual understanding.\n",
      "\n",
      "3. **Transformer Architecture:**\n",
      "   - **Key Mechanism:** Self-attention mechanism\n",
      "     - **Definition:** Allows a model to weigh the importance of different words in a sentence when processing it.\n",
      "     - **Mechanism:** Each word attends to all others via scaled dot-product attention, enabling capturing long-range dependencies and context efficiently.\n",
      "     - **Efficiency:** Parallel processing allows simultaneous consideration of all context windows, unlike RNNs, accelerating training.\n",
      "\n",
      "4. **Self-Attention:**\n",
      "   - **Functionality:** Enables words in a sentence to attend to various other positions, highlighting relevant contexts dynamically.\n",
      "   - **Parallel Processing:** Unlike sequential models, transformers process the entire sequence at once, enhancing efficiency and capturing global dependencies effectively.\n",
      "\n",
      "5. **Multi-head Attention:**\n",
      "   - **Enhancement Strategy:** Divides attention computation into multiple heads, each focusing on different aspects, improving model expressive power without increasing complexity significantly.\n",
      "\n",
      "6. **Neural Network Training:**\n",
      "   - **Process:** Involves forward propagation (input to output) and backpropagation (error correction), optimizing parameters through iterative adjustments to minimize prediction errors using cost functions.\n",
      "\n",
      "7. **LLMs Utilization:**\n",
      "   - **Applications:** These advanced model architectures enable LLMs to excel in tasks like text generation, translation, summarization, requiring extensive training data for context-rich performance.\n",
      "\n",
      "In summary, neural networks form the backbone of LLMs through layers of processing, while self-attention mechanisms (with multi-head configurations) allow efficient and comprehensive handling of contextual information. Transformers revolutionize how models process sequential data, driving advancements in language understanding and generation capabilities.\n"
     ]
    }
   ],
   "source": [
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=\"deepseek-r1:latest\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Please give definitions of some core concepts behind LLMs: a neural network, attention and the transformer\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
